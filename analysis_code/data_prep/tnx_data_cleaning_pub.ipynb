{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-09T11:38:56.455773Z",
     "start_time": "2023-05-09T11:38:50.011163Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data science\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from statsmodels.stats.multitest import multipletests as mt\n",
    "\n",
    "# Plots\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import to_hex\n",
    "\n",
    "# Working with dates\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import dateutil\n",
    "\n",
    "# Looping  progress\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Reg expressions\n",
    "import re\n",
    "\n",
    "# Pretty table printing\n",
    "import tabulate\n",
    "\n",
    "# ***REMOVED*** Snippets Require these\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Misc libraries\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "# Set seaborn figure size, font size, and style\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# Set Pandas options so we can see our entire dataframe\n",
    "pd.options.display.max_rows = 10000\n",
    "pd.options.display.max_columns = 10000\n",
    "pd.options.display.max_colwidth = None\n",
    "\n",
    "# Print our versions of this packages, this allows us to make sure\n",
    "# we have the working versions we need. \n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"***REMOVED***/other/trinetx/new_dataset\"\n",
    "HOME = \"***REMOVED***\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General data manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because a lot of these files were large, a lot of the initial handling happened using BASH and different CLI tools, especially GNU Parallel. I'll try to document everything that was done here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Decrypting and Decompressing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```bash\n",
    "# Move to our main directory\n",
    "cd $BASE_DIR\n",
    "\n",
    "# Assume zip file from TNX is called dataset_abc.zip\n",
    "unzip dataset_abc.zip\n",
    "\n",
    "mv dataset_abc new_dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnoses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Diagnosis data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first step is to break the ICD codes up into 3-character and post 'decimal' bit. --pipepart will break the file input to GNU parallel (-a flag) into equal chunks based on the block size.\n",
    "\n",
    "* The AWK command needs to survive GNU parallel's argument processing and still be surrounded by single quotes, so an extra layer of quotes (and some escaping) is required.\n",
    " \n",
    "* The AWK program itself is splitting the 4th column, the ICD code on the decimal and then reprinting the original line plus a new column for the 3-char ICD code and a new column for the post-decimal section.\n",
    "\n",
    "```bash\n",
    "# Move into our directory\n",
    "BASE_DIR=\"${BASE_DIR}/new_dataset\"\n",
    "cd \"${BASE_DIR}\"\n",
    "\n",
    "# Call the parallel program\n",
    "parallel --jobs 16 --block 1G --pipepart -a diagnosis.csv  awk -F, -vOFS=, \"'{ split(\\$4, a, \\\".\\\"); print \\$0, a[1], a[2]}'\" > diagnosis_3_char.csv\n",
    "```\n",
    "  \n",
    "  \n",
    "After splitting out the 3-character ICD code we want to sort by the patient ID (column 1), the 3-character ICD code (column 11), and the date (column 8), so that all diagnoses for each patient are sorted within patient and within ICD code by date, with the oldest diagnosis instance at the top of the file.\n",
    "\n",
    "```bash\n",
    "# Parsort is written by Ole Tange and attached GNU parallel and sort\n",
    "parsort --parallel=16 -t ','  -k1,1 -k11,11 -k8,8 diagnosis_3_char.csv > diagnosis_3_char_sorted.csv\n",
    "```\n",
    "\n",
    "Now to do some exploration, I wanted to look at the distribution of a few values\n",
    "* First just looking at the source ID \n",
    "\n",
    "```bash\n",
    "awk -F, '{print $10}' diagnosis_3_char_sorted.csv | sort | uniq -c > src_count\n",
    "\n",
    "head src_count | column -t\n",
    "1861535170  \"EHR\"\n",
    "300089434   \"NLP\"\n",
    "1           \"source_id\"\n",
    "253158      \"TriNetX\"\n",
    "```\n",
    "\n",
    "So it looks like most of the diagnoses are straight from the EHR, but 300M are from NLP so we need some kind of rules here. For now we will only use EHR data.\n",
    "* EHR_ONLY: Only include EHR diagnoses\n",
    "* ALL_SRC: Include diagnoses from all sources\n",
    "* EHR_DATE_FIRST: Include diagnoses from all sources but use the EHR date of diagnosis for each disease, unless there is not an EHR diagnosis for that code in which case, use the earliest date, either from NLP or TriNetX.\n",
    "\n",
    "```bash\n",
    "# Only print out columns where the 10th column (the source column) is EHR.\n",
    "awk -F, '$10 == \"\\\"EHR\\\"\"' diagnosis_3_char_sorted.csv > diagnosis_3_char_sorted_ehr_only.csv\n",
    "```\n",
    "\n",
    "I also noted there are more than just ICD10 codes, so I wanted to investigate what we have there. It looks like just under 68% of all diagnoses are in ICD10, which is good because that's all we have codes for in UKB without doing some sort of crazy translation, which isn't super straightforward unfortunately. So for now we will be ignoring the ICD9 codes.\n",
    "\n",
    "```bash\n",
    "# Get the counts for each type of ICD coding scheme\n",
    "\n",
    "parallel --jobs 16 --block 1G --pipepart -a diagnosis_3_char_sorted_ehr_only.csv \\\n",
    "         awk -F, \"'{print \\$3}'\" > icd_code.list  &&\n",
    "  parsort --parallel=16 -t, icd_code.list > icd_code_sorted.list &&\n",
    "  uniq -c icd_code_sorted.list > icd_code_counts &&\n",
    "  sort -k1 -nr icd_code_counts | sponge icd_code_counts\n",
    "\n",
    "# 1262909432 \"ICD-10-CM\"\n",
    "# 598622778 \"ICD-9-CM\"\n",
    "#   2960 \"\"\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting diagnoses into 3-char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way I could think to further process the diagnosis data was to split it into much more manageable chunks that we could read and process with Python. To do this, I figured it would be easiest to just split each of the ICD10 codes we are interested in from UKB into a separate file. To do this I'll be using a [submitArrayJobs](https://github.com/ernstki/submitArrayJobs) call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T19:17:42.146287Z",
     "start_time": "2023-08-22T19:17:42.134508Z"
    }
   },
   "source": [
    "<hr>\n",
    "Create the break_diags.sh script to filter for each ICD10 code listed in the array file, as we will be using break_diags.sh as our command file.\n",
    "\n",
    "```bash\n",
    "cat break_diags.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "OUT_DIR=\"${BASE_DIR}/icd_data\"\n",
    "\n",
    "# ICD code from array file inserted by submitArrayJobs prior to be submitted to HPC.\n",
    "curr_icd=VAR1\n",
    "curr_fn=\"${OUT_DIR}/${curr_icd}_only.csv\"\n",
    "\n",
    "\n",
    "# Need to export so GNU Parallel can see it\n",
    "export curr_icd\n",
    "\n",
    "\n",
    "parallel --jobs 16 --block 1G --pipepart -a diagnosis_3_char_sorted.csv  awk -v icd=\\\"\\$curr_icd\\\" -F\\\",\\\" \\'\\$11 \\~ icd \\{print \\$0\\;\\}\\' > \"${curr_fn}\"\n",
    "```\n",
    "\n",
    "<hr>\n",
    "\n",
    "The array file is simply a list of the ICD codes we are interested in:\n",
    "```bash\n",
    "head af\n",
    "A60     A60\n",
    "B02     B02\n",
    "B19     B19\n",
    "B24     B24\n",
    "...\n",
    "```\n",
    "\n",
    "<hr>\n",
    "\n",
    "Then we can just submit these jobs using submitArrayJobs\n",
    "\n",
    "```bash\n",
    "submitArrayJobs -C break_diags.sh -A af -W 24:00 -n 16 -M 200000\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'll show how we processed all the lab data, both categorical and continuous, but it's important to note that we decided to drop the continuous tests and only use categorical. We will again use submitArrayJobs (sAJ) for submitting a separate job to look through the huge (~700 GB) file of lab tests for each LOINC code of interest.\n",
    "\n",
    "\n",
    "```bash\n",
    "# break_labs_new.sh\n",
    "#!/bin/bash\n",
    "\n",
    "OUT_DIR=\"${BASE_DIR}/lab_data\"\n",
    "\n",
    "# sAJ will insert the proper LOINC code in curr_loinc\n",
    "curr_loinc=VAR1\n",
    "curr_fn=\"${OUT_DIR}/${curr_loinc}_only_single_thread.csv\"\n",
    "\n",
    "# Need to export so GNU Parallel can see it\n",
    "export curr_loinc\n",
    "\n",
    "awk -v loinc=\"$curr_loinc\" -F\",\" '$4 ~ loinc {print $0;}' lab_result.csv   > \"${curr_fn}\"\n",
    "\n",
    "```\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "```bash\n",
    "head labs_af\n",
    "loinc_63475-8   63475-8\n",
    "loinc_26645-2   26645-2\n",
    "loinc_62466-8   62466-8\n",
    "loinc_33978-8   33978-8\n",
    "loinc_90924-2   90924-2\n",
    "...\n",
    "```\n",
    "\n",
    "<hr>\n",
    "\n",
    "```bash\n",
    "submitArrayJobs -C break_labs_new.sh -A labs_af -W 24:00 -n 16 -M 200000\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Generate file that summarizes the results we have for all of the labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "\n",
    "meta_dir = BASE_DIR\n",
    "lab_counts = pd.read_csv(f\"{meta_dir}/clean_loinc_counts.tsv\", sep = '\\t')\n",
    "\n",
    "lab_dir = f\"{BASE_DIR}/lab_data\"\n",
    "lab_files = glob.glob(f\"{lab_dir}/*.csv\")\n",
    "\n",
    "REQ_LAB_NUM = 20\n",
    "\n",
    "res_ls = []\n",
    "\n",
    "for curr_lab_ind in tqdm(range(0, len(lab_files), 1)):\n",
    "    curr_lab_fn = lab_files[curr_lab_ind]\n",
    "    fn_loinc = os.path.basename(curr_lab_fn).split('_')[0]\n",
    "\n",
    "    curr_dat = pd.read_csv(curr_lab_fn, sep = ',', \n",
    "                           names = ['pat_id', 'enc_id', 'LOINC', 'code', 'date', \n",
    "                                     'num_value', 'text_value', 'units', 'Tri_deriv',\n",
    "                                      'src_id'], \n",
    "                           index_col=False, quoting=csv.QUOTE_NONE)\n",
    "    curr_dat = curr_dat.applymap(lambda x: str(x).lstrip('\"').rstrip('\"'))\n",
    "\n",
    "    \n",
    "    curr_dat = curr_dat.loc[curr_dat['code'] == fn_loinc, :]\n",
    "    \n",
    "    if len(curr_dat) < REQ_LAB_NUM:\n",
    "        print(f\"{fn_loinc}: only {len(curr_dat)} people, skipping analysis!\")\n",
    "        continue\n",
    "\n",
    "    curr_loinc = curr_dat['code'].unique()[0]\n",
    "\n",
    "    if fn_loinc != curr_loinc:\n",
    "        print(f\"{fn_loinc}: LOINC mismatch, fn: {fn_loinc}, data: {curr_loinc}, skipping analysis!\")\n",
    "\n",
    "\n",
    "    test_info = lab_counts.loc[lab_counts['loinc'] == curr_loinc, :]\n",
    "\n",
    "    n_uniq_pats = test_info['count'].tolist()[0]\n",
    "\n",
    "    if n_uniq_pats < REQ_LAB_NUM:\n",
    "        print(f\"{curr_loinc}: only {n_uniq_pats} people, skipping analysis!\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    test_scale = test_info['SCALE_TYP'].tolist()[0]\n",
    "\n",
    "    units = test_info['unit'].tolist()[0]\n",
    "\n",
    "    if (test_scale == 'Ord') and (units != 'boolean'):\n",
    "        print(f\"{curr_loinc}: 'Ord' test with non-boolean units!\")\n",
    "\n",
    "    nrow = len(curr_dat)\n",
    "    n_pats = len(curr_dat.loc[:, 'pat_id'].unique().tolist())\n",
    "\n",
    "    units_dict = curr_dat['units'].value_counts().to_dict()\n",
    "\n",
    "    non_null = curr_dat.loc[((curr_dat['num_value'].notnull()) &\n",
    "                          (curr_dat['num_value'] != '') &\n",
    "                          (curr_dat['num_value'] != 'nan')), :].copy(deep = True)\n",
    "\n",
    "\n",
    "    n_null = len(curr_dat.loc[((curr_dat['num_value'].isnull()) |\n",
    "                             (curr_dat['num_value'] == '') |\n",
    "                              (curr_dat['num_value'] == 'nan')), :])\n",
    "    \n",
    "    non_null.loc[:, 'num_value'] = non_null.loc[:, 'num_value'].astype('float')\n",
    "    \n",
    "\n",
    "    n_vals_0 = sum(non_null['num_value'] == 0)\n",
    "    #print(f\"{curr_loinc}: {n_vals_0}\")\n",
    "    n_vals_non_0 = sum(non_null['num_value'] != 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    n_unique = len(non_null['num_value'].unique())\n",
    "\n",
    "    #curr_val = non_null['num_value'].value_counts(dropna = False).to_dict()\n",
    "\n",
    "\n",
    "    num_lt_0 = len(non_null.loc[non_null['num_value'] < 0, :])\n",
    "    num_mean = non_null['num_value'].mean()\n",
    "    num_sd = non_null['num_value'].std()\n",
    "    num_med = non_null['num_value'].median()\n",
    "    num_min = non_null['num_value'].min()\n",
    "    num_max = non_null['num_value'].max()\n",
    "\n",
    "    non_null_no_zero = non_null.loc[non_null['num_value'] > 0, :]\n",
    "    if len(non_null_no_zero) > 0:\n",
    "        bins = [0, 1, 10, 100, 1000, 10000, 1e5, 1e7, 1e8]\n",
    "\n",
    "        tens = pd.DataFrame(non_null_no_zero['num_value'].value_counts(bins = bins, sort = False)).reset_index()\n",
    "        tens_str = tens.to_string(header = False, index = False)\n",
    "\n",
    "\n",
    "\n",
    "        num_vals = pd.DataFrame(non_null_no_zero['num_value'].value_counts(bins = 20, sort = False)).reset_index()\n",
    "        num_vals_str = num_vals.to_string(header = False, index = False)\n",
    "    else:\n",
    "        tens_str = ''\n",
    "        num_vals_str = ''\n",
    "\n",
    "\n",
    "    n_cat_with_value = len(curr_dat.loc[((curr_dat['text_value'].notnull()) & \n",
    "                            (curr_dat['text_value'] != '')), 'pat_id'].unique().tolist())\n",
    "\n",
    "    non_null = curr_dat.loc[((curr_dat['text_value'].notnull()) & \n",
    "                            (curr_dat['text_value'] != '')), :]\n",
    "\n",
    "    curr_val_con = curr_dat['text_value'].value_counts(dropna = False).to_dict()\n",
    "\n",
    "\n",
    "    res_ls.append([curr_loinc, REQ_LAB_NUM, n_uniq_pats, test_scale, units,\n",
    "                   nrow, n_pats, units_dict, n_null, n_vals_0, n_vals_non_0,\n",
    "                   n_unique, num_lt_0, num_mean, num_sd, num_med,\n",
    "                   num_min, num_max, tens_str, num_vals_str,\n",
    "                   n_cat_with_value, curr_val_con])\n",
    "    \n",
    "res = pd.DataFrame(res_ls, columns = ['loinc', 'REQ_NUM', 'num_uniq_pats',\n",
    "                                     'test_scale', 'test_units', 'nrow',\n",
    "                                     'n_pats', 'units_dict', 'n_num_null',\n",
    "                                     'n_num_vals_0', 'n_num_vals_non_0',\n",
    "                                     'n_num_unique', \n",
    "                                      'n_lt_0', 'mean', 'sd', 'med', 'min', 'max',\n",
    "                                      'tens_dict', 'hist_dict', 'n_cat_w_val',\n",
    "                                     'cat_vals_dict'])\n",
    "\n",
    "\n",
    "res.to_csv(f\"{BASE_DIR}/lab_test_data_analysis.tsv\", sep = \"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<hr>\n",
    "The labs results were then manually checked to see which categorical LOINC codes had enough results (negative or positive) for us to be powered to run an analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating disease-pathogen pair data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A python script was used to create a set of data pairing each ICD10 code with each LOINC code. Prior to running, we removed the 7 ICD10 codes from all_icd_af that have no cases in TNX [E14, I64, I84, J46, K07, K10, O81].\n",
    "\n",
    "  \n",
    "We call our python script directly from the command file. This python script, trinetx_pairs_single_icd_pub.py can be found elsewhere in this repository. But again we are using sAJ to generate all pairs for each ICD10 code.\n",
    "\n",
    "<hr> \n",
    "\n",
    "```bash\n",
    "cf.sh\n",
    "#!/bin/bash\n",
    "\n",
    "module purge\n",
    "module load python3/3.10.7\n",
    "\n",
    "python \"${HOME}/code/antigen_research/trinetx/trinetx_pairs_single_icd_pub.py\" --icd VAR1\n",
    "\n",
    "```\n",
    "\n",
    "<hr> \n",
    "\n",
    "```bash\n",
    "head af\n",
    "A60_trinetx_pairs       A60\n",
    "B00_trinetx_pairs       B00\n",
    "B02_trinetx_pairs       B02\n",
    "B19_trinetx_pairs       B19\n",
    "B24_trinetx_pairs       B24\n",
    "...\n",
    "```\n",
    "<hr> \n",
    "\n",
    "```bash\n",
    "submitArrayJobs  -C cf.sh -A af  -W 24:00 -n 1 -M 20000\n",
    "```\n",
    "\n",
    "<hr>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Many checks were completed to verify that all jobs ran successfully and didn't fail part way through for all parts of TNX data processing. These checks are just not shown here, as they don't seem particular relevant.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-22T19:42:25.515131Z",
     "start_time": "2023-08-22T19:42:25.508571Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Patient data / Covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Load in patient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Now look at each of the UKB covariates to see if we can extract data for them in the TNX data\n",
    "pats = pd.read_csv(f\"{BASE_DIR}/patient.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:13:40.525145Z",
     "start_time": "2023-02-28T14:13:40.164357Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# F          7917303\n",
    "# M          4177790\n",
    "# Unknown       7061\n",
    "pats['sex'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:13:44.528131Z",
     "start_time": "2023-02-28T14:13:40.717022Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Encode like UKB (F = 0)\n",
    "pats.loc[:, 'sex'] = pats.loc[:, 'sex'].replace({'F' : 0,\n",
    "                            'M' : 1,\n",
    "                            'Unknown' : 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:13:44.685154Z",
     "start_time": "2023-02-28T14:13:44.638030Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# new dataset\n",
    "# 0    7917303\n",
    "# 1    4177790\n",
    "# 2       706\n",
    "pats['sex'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## BMI - SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Only have BMI data for 37.3% of participants, so we can't really\n",
    "# impute here, so we will just have to drop BMI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T19:01:51.827636Z",
     "start_time": "2023-02-28T19:01:51.811915Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TriNetX data was made available on Feb 13, 2023, so that's the latest\n",
    "# we should have data for, so calculate age to that date, unless the \n",
    "# participant is dead, in which case set age to age at death as that's when\n",
    "# their exposure ended.\n",
    "# However, since TNX only has year of birth resolution, people that were born\n",
    "# in 2023, if we calculated age using year data was made available of 2023, the\n",
    "# age would be 0, so we will round up the year the data was made available to\n",
    "# 2024, so the youngest people will be 1 yr old.\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "def calc_curr_age(curr_pat_row):\n",
    "    #print(curr_pat_row)\n",
    "    \n",
    "    # Year data was released to us (rounded up)\n",
    "    CURR_YEAR = 2023\n",
    "\n",
    "    curr_id = curr_pat_row[0]\n",
    "    curr_yob = curr_pat_row[5]\n",
    "    curr_death_str = curr_pat_row[7]\n",
    "\n",
    "    is_dead = pd.notnull(curr_death_str)\n",
    "\n",
    "    #print(f'{curr_id}: {curr_yob}  -> {curr_death_str}')\n",
    "\n",
    "    # If they are dead\n",
    "    if is_dead:\n",
    "\n",
    "        # Remove trailing decimal (got added at some point)\n",
    "        curr_death_str = str(int(curr_death_str))\n",
    "\n",
    "        curr_death = dt.strptime(str(curr_death_str), '%Y%m')\n",
    "\n",
    "        # Midpoint of year is July 2nd (unless leap year in which case it's July 1)\n",
    "        # But we only have year and month, so July is midpoint of year\n",
    "\n",
    "        # Round to next year\n",
    "        if curr_death.month >= 6:\n",
    "            death_yr = curr_death.year + 1\n",
    "\n",
    "        else:\n",
    "            death_yr = curr_death.year\n",
    "\n",
    "        curr_age = death_yr - curr_yob\n",
    "\n",
    "    # Still alive\n",
    "    else:\n",
    "        curr_age = CURR_YEAR - curr_yob\n",
    "        \n",
    "        \n",
    "    return curr_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T19:04:22.849205Z",
     "start_time": "2023-02-28T19:01:53.253482Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Allows us to show progress bar during apply operations\n",
    "tqdm_notebook().pandas()\n",
    "\n",
    "pats['age'] = pats.progress_apply(calc_curr_age, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T19:06:31.490817Z",
     "start_time": "2023-02-28T19:06:21.977540Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Participants without age values (86,790) will be dropped from dynamically generated\n",
    "# cohorts when the UKB model we are refitting requires adjustment for age. If the UKB\n",
    "# model does not adjust for age, then we can keep these people in the cohort.\n",
    "\n",
    "# Age\n",
    "# Unique Patients: 12102154\n",
    "# Mean:   46.352431353723446\n",
    "# Stdev:  17.465842251088695\n",
    "# Median: 44.0\n",
    "# Min:    0.0\n",
    "# Max:    91.0\n",
    "# Age = NA: 86790\n",
    "# Age < 0: 0\n",
    "# Age = 0: 72\n",
    "    \n",
    "print(\"Age\")\n",
    "print(f\"Unique Patients: {len(pats['patient_id'].unique())}\")\n",
    "print(f\"Mean:   {pats['age'].mean()}\")\n",
    "print(f\"Stdev:  {pats['age'].std()}\")\n",
    "print(f\"Median: {pats['age'].median()}\")\n",
    "print(f\"Min:    {pats['age'].min()}\")\n",
    "print(f\"Max:    {pats['age'].max()}\")\n",
    "print(f\"Age = NA: {sum(pats['age'].isna())}\")\n",
    "print(f\"Age < 0: {len(pats.loc[pats['age']  < 0, :])}\")\n",
    "print(f\"Age = 0: {len(pats.loc[pats['age']  == 0, :])}\")\n",
    "\n",
    "\n",
    "# Scale the age  (divide by 10)\n",
    "pats['s_age'] = pats['age']/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-25T19:48:42.481193Z",
     "start_time": "2022-10-25T19:48:42.448578Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Race/ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T19:06:32.472952Z",
     "start_time": "2023-02-28T19:06:31.865165Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# White                                        5538331\n",
    "# Unknown                                      4388318\n",
    "# Black or African American                    1834228\n",
    "# Asian                                         280490\n",
    "# American Indian or Alaska Native               44547\n",
    "# Native Hawaiian or Other Pacific Islander      16240\n",
    "pats['race'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T19:15:04.832057Z",
     "start_time": "2023-02-28T19:15:04.152089Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Not Hispanic or Latino    6303947\n",
    "# Unknown                   4895691\n",
    "# Hispanic or Latino         902516\n",
    "pats['ethnicity'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T19:26:12.510000Z",
     "start_time": "2023-02-28T19:26:10.690415Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# For UKB analysis any mixed race such as 'White and black caribbean' were\n",
    "# coded as 'other', but for TNX there is a distinct 'hispanic' group, so \n",
    "# we will code, any ethnicity that is hispanic/latino as hispanic.\n",
    "#\n",
    "# \n",
    "# race                                       ethnicity             \n",
    "# American Indian or Alaska Native           Hispanic or Latino           6219\n",
    "#                                            Not Hispanic or Latino      22764\n",
    "#                                            Unknown                     15564\n",
    "# Asian                                      Hispanic or Latino           3673\n",
    "#                                            Not Hispanic or Latino     226936\n",
    "#                                            Unknown                     49881\n",
    "# Black or African American                  Hispanic or Latino          24876\n",
    "#                                            Not Hispanic or Latino    1459988\n",
    "#                                            Unknown                    349364\n",
    "# Native Hawaiian or Other Pacific Islander  Hispanic or Latino           2729\n",
    "#                                            Not Hispanic or Latino      10463\n",
    "#                                            Unknown                      3048\n",
    "# Unknown                                    Hispanic or Latino         408834\n",
    "#                                            Not Hispanic or Latino     318647\n",
    "#                                            Unknown                   3660837\n",
    "# White                                      Hispanic or Latino         456185\n",
    "#                                            Not Hispanic or Latino    4265149\n",
    "#                                            Unknown                    816997\n",
    "pats.value_counts(['race', 'ethnicity'], dropna = False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T19:33:44.863332Z",
     "start_time": "2023-02-28T19:33:31.779944Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Changing to if ethnicity is unknown you are classified to race response\n",
    "# 'Asian, Unknown' -> 'Asian'\n",
    "# Default to blank but that should get wiped out completely\n",
    "pats['fin_race'] = 'blank'\n",
    "\n",
    "pats.loc[((pats['race'] == 'White') & \n",
    "          ((pats['ethnicity'] == 'Not Hispanic or Latino') |\n",
    "          (pats['ethnicity'] == 'Unknown'))), 'fin_race'] = 'white'\n",
    "\n",
    "pats.loc[((pats['race'] == 'Asian') & \n",
    "          ((pats['ethnicity'] == 'Not Hispanic or Latino') |\n",
    "          (pats['ethnicity'] == 'Unknown'))), 'fin_race'] = 'asian'\n",
    "\n",
    "\n",
    "pats.loc[((pats['race'] == 'Black or African American') & \n",
    "          ((pats['ethnicity'] == 'Not Hispanic or Latino') |\n",
    "          (pats['ethnicity'] == 'Unknown'))), 'fin_race'] = 'black'\n",
    "\n",
    "pats.loc[((pats['race'] == 'American Indian or Alaska Native') & \n",
    "          ((pats['ethnicity'] == 'Not Hispanic or Latino') |\n",
    "          (pats['ethnicity'] == 'Unknown'))), 'fin_race'] = 'ai_an'\n",
    "\n",
    "pats.loc[((pats['race'] == 'Native Hawaiian or Other Pacific Islander') & \n",
    "          ((pats['ethnicity'] == 'Not Hispanic or Latino') |\n",
    "          (pats['ethnicity'] == 'Unknown'))), 'fin_race'] = 'nh_opi'\n",
    "\n",
    "pats.loc[((pats['race'] == 'Unknown') & \n",
    "          ((pats['ethnicity'] == 'Not Hispanic or Latino') |\n",
    "          (pats['ethnicity'] == 'Unknown'))), 'fin_race'] = 'other'\n",
    "\n",
    "pats.loc[pats['ethnicity'] == 'Hispanic or Latino', 'fin_race'] = 'hispanic'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T19:33:45.679571Z",
     "start_time": "2023-02-28T19:33:45.288108Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Counts are good:\n",
    "# ai_an         22764\n",
    "# asian        226936\n",
    "# black       1459988\n",
    "# hispanic     902516\n",
    "# nh_opi        10463\n",
    "# other       5214338\n",
    "# white       4265149\n",
    "pats['fin_race'].value_counts(dropna = False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T19:38:50.564942Z",
     "start_time": "2023-02-28T19:38:46.006840Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# UKB variable is called 'ethnic' so let's use that for final code\n",
    "pats['ethnic'] = pats.loc[:, 'fin_race'].replace({\n",
    "                                                    'white'    : 0,\n",
    "                                                    'asian'    : 1,\n",
    "                                                    'black'    : 2, \n",
    "                                                    'hispanic' : 3,\n",
    "                                                    'ai_an'    : 4,\n",
    "                                                    'nh_opi'   : 5,\n",
    "                                                    'other'    : 6\n",
    "\n",
    "                                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T19:38:53.034732Z",
     "start_time": "2023-02-28T19:38:52.961374Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 0    5082146\n",
    "# 1     276817\n",
    "# 2    1809352\n",
    "# 3     902516\n",
    "# 4      38328\n",
    "# 5      13511\n",
    "# 6    3979484\n",
    "pats['ethnic'].value_counts(dropna = False).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Townsend Deprivation Index - SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# No deprivation index available in TNX data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Number in house - SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# No number in house is available in TNX data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Tobacco Use - SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# No reliable source of tobacco usage could be found in available TNX data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Alcohol Use - SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# No reliable source of alcohol usage could be found in available TNX data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Number of Sex Partners - SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# No number of sex partners is available in TNX data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Same-sex Intercourse - SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# No same-sex intercourse is available in TNX data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Save covariate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T13:26:53.492830Z",
     "start_time": "2023-03-01T13:26:53.352864Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pats = pats.set_index('patient_id')\n",
    "pats = pats.loc[:, ['sex', 'ethnic', 'age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T13:32:29.454875Z",
     "start_time": "2023-03-01T13:32:29.387196Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#  86K missing age, we will try to impute those but will also\n",
    "#              keep a column with NAs for missing ages\n",
    "# Nan in each columns\n",
    "# sex           0\n",
    "# ethnic        0\n",
    "# age       86790\n",
    "# get number of NAs in each column\n",
    "print(\"Nan in each columns\" , pats.isnull().sum(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T13:36:28.433538Z",
     "start_time": "2023-03-01T13:36:14.427321Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create the imputer object\n",
    "MICE = IterativeImputer(random_state=0)\n",
    "\n",
    "# do the actual imputation\n",
    "ret = MICE.fit_transform(pats)\n",
    "\n",
    "# Convert imputed data from np array to df\n",
    "ret_df = pd.DataFrame(ret)\n",
    "\n",
    "# Reset the column names and indices\n",
    "ret_df.columns = pats.columns\n",
    "ret_df.index = pats.index\n",
    "\n",
    "ret_df = ret_df.loc[:, ['age']].copy(deep = True)\n",
    "ret_df.columns = ['imp_age']\n",
    "\n",
    "pats = pats.merge(ret_df, left_index = True, right_index = True, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T13:38:00.407081Z",
     "start_time": "2023-03-01T13:38:00.319888Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# New dataset: 86K missing age, but now we have imputed age with nothing missing\n",
    "# Nan in each columns\n",
    "# sex            0\n",
    "# ethnic         0\n",
    "# age        86790\n",
    "# imp_age        0\n",
    "# get number of NAs in each column\n",
    "print(\"Nan in each columns\" , pats.isnull().sum(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T13:40:29.955630Z",
     "start_time": "2023-03-01T13:40:29.383775Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pats = pats.reset_index(drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T13:42:02.296375Z",
     "start_time": "2023-03-01T13:41:46.699842Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sex:\n",
    "# 0    7917303\n",
    "# 1    4177790\n",
    "# 2       7061\n",
    "# Name: sex, dtype: int64\n",
    "# ==================================\n",
    "# Ethnic:\n",
    "# 0    5082146\n",
    "# 1     276817\n",
    "# 2    1809352\n",
    "# 3     902516\n",
    "# 4      38328\n",
    "# 5      13511\n",
    "# 6    3979484\n",
    "# Name: ethnic, dtype: int64\n",
    "# ==================================\n",
    "# Age\n",
    "# Unique Patients: 12102154\n",
    "# Mean:   46.352431353723446\n",
    "# Stdev:  17.465842251088695\n",
    "# Median: 44.0\n",
    "# Min:    0.0\n",
    "# Max:    91.0\n",
    "# Age = NA: 86790\n",
    "# Age < 0: 0\n",
    "# Age = 0: 72\n",
    "# ==================================\n",
    "# Imp Age\n",
    "# Unique Patients: 12102154\n",
    "# Mean:   46.35241817051056\n",
    "# Stdev:  17.404778331928817\n",
    "# Median: 44.0\n",
    "# Min:    0.0\n",
    "# Max:    91.0\n",
    "# Age = NA: 0\n",
    "# Age < 0: 0\n",
    "# Age = 0: 72\n",
    "\n",
    "print(f\"Sex:\")\n",
    "print(pats['sex'].value_counts(dropna = False).sort_index())\n",
    "print(\"==================================\")\n",
    "\n",
    "\n",
    "print(f\"Ethnic:\")\n",
    "print(pats['ethnic'].value_counts(dropna = False).sort_index())\n",
    "print(\"==================================\")\n",
    "\n",
    "\n",
    "print(\"Age\")\n",
    "print(f\"Unique Patients: {len(pats['patient_id'].unique())}\")\n",
    "print(f\"Mean:   {pats['age'].mean()}\")\n",
    "print(f\"Stdev:  {pats['age'].std()}\")\n",
    "print(f\"Median: {pats['age'].median()}\")\n",
    "print(f\"Min:    {pats['age'].min()}\")\n",
    "print(f\"Max:    {pats['age'].max()}\")\n",
    "print(f\"Age = NA: {sum(pats['age'].isna())}\")\n",
    "print(f\"Age < 0: {len(pats.loc[pats['age']  < 0, :])}\")\n",
    "print(f\"Age = 0: {len(pats.loc[pats['age']  == 0, :])}\")\n",
    "print(\"==================================\")\n",
    "\n",
    "\n",
    "print(\"Imp Age\")\n",
    "print(f\"Unique Patients: {len(pats['patient_id'].unique())}\")\n",
    "print(f\"Mean:   {pats['imp_age'].mean()}\")\n",
    "print(f\"Stdev:  {pats['imp_age'].std()}\")\n",
    "print(f\"Median: {pats['imp_age'].median()}\")\n",
    "print(f\"Min:    {pats['imp_age'].min()}\")\n",
    "print(f\"Max:    {pats['imp_age'].max()}\")\n",
    "print(f\"Age = NA: {sum(pats['imp_age'].isna())}\")\n",
    "print(f\"Age < 0: {len(pats.loc[pats['imp_age']  < 0, :])}\")\n",
    "print(f\"Age = 0: {len(pats.loc[pats['imp_age']  == 0, :])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-01T13:42:53.953340Z",
     "start_time": "2023-03-01T13:42:53.635514Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Scale ages before saving output\n",
    "pats.loc[:, 'age'] = pats.loc[:, 'age'] / 10\n",
    "pats.loc[:, 'imp_age'] = pats.loc[:, 'imp_age'] / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T13:03:56.904568Z",
     "start_time": "2023-03-20T13:03:25.713573Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Save the file\n",
    "pats.to_csv(f\"{BASE_DIR}/procd_data/procd_covs.tsv\", sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Healthy Pregnancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-04T00:30:27.013175Z",
     "start_time": "2023-03-04T00:30:24.652478Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# We need to get a list of the particpants who have had a healthy birth as our control\n",
    "# group for any ICD10 O codes.\n",
    "HEALTHY_PREGNANCY_ICD_CODES = [\"O80\", \"O81\", \"O82\", \"O83\", \"O84\"]\n",
    "\n",
    "icd_dir = f'{BASE_DIR}/icd_data'\n",
    "\n",
    "\n",
    "diag_cols = ['pat_id', 'enc_id', 'vocab', 'full_code',\n",
    "             'principal_diag_indicator', 'admit_diag',\n",
    "             'reason_for_visit', 'date', 'derived_by_tri',\n",
    "             'source_id', 'icd_3_char', 'icd_sub_cat']\n",
    "\n",
    "dtype_dict = {'pat_id': str, 'enc_id': str, 'vocab': str, 'full_code': str,\n",
    "              'principal_diag_indicator': str, 'admit_diag': str,\n",
    "              'reason_for_visit': str, 'date': str,\n",
    "              'derived_by_tri': str, 'source_id': str, 'icd_3_char': str,\n",
    "              'icd_sub_cat': str}\n",
    "\n",
    "fin_dat = pd.DataFrame(columns = diag_cols)\n",
    "\n",
    "for curr_icd in tqdm(HEALTHY_PREGNANCY_ICD_CODES):\n",
    "    icd_fn = f\"{icd_dir}/{curr_icd}_only.csv\"\n",
    "    \n",
    "    if not os.path.exists(icd_fn):\n",
    "        print(f\"No ICD data file for {curr_icd}... skipping\")\n",
    "        continue\n",
    "        \n",
    "    curr_dat = pd.read_csv(icd_fn, names = diag_cols, dtype = dtype_dict)\n",
    "    fin_dat = pd.concat([fin_dat, curr_dat], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-04T00:31:50.406508Z",
     "start_time": "2023-03-04T00:31:50.135718Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fin_dat = fin_dat.drop_duplicates('pat_id').loc[:, ['pat_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-04T00:31:59.776415Z",
     "start_time": "2023-03-04T00:31:59.083957Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Save the file out\n",
    "fin_dat.to_csv(f'{BASE_DIR}/procd_data/healthy_pregnancy_data.tsv',\n",
    "               sep = '\\t', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
